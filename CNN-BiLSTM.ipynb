{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## half day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = pd.read_csv('use.csv')\n",
    "# Convert to datetime format\n",
    "data['connectionTime'] = pd.to_datetime(data['connectionTime'])\n",
    "\n",
    "# Set connectionTime as index\n",
    "data.set_index('connectionTime', inplace=True)\n",
    "\n",
    "# Sum up the kWhDelivered per half day\n",
    "data_grouped = data['kWhDelivered'].resample('12H').sum().reset_index()\n",
    "data_grouped['connectionTime'] = pd.to_datetime(data_grouped['connectionTime'])\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_grouped['scaled_kWhDelivered'] = scaler.fit_transform(data_grouped['kWhDelivered'].values.reshape(-1, 1))\n",
    "\n",
    "# Now you have data_grouped with half-day kWhDelivered sums and normalized values\n",
    "# Add day_of_week as a cyclical feature\n",
    "data_grouped['day_of_week'] = data_grouped['connectionTime'].dt.dayofweek\n",
    "\n",
    "# Convert the 'connectionTime' to datetime\n",
    "data_grouped['connectionTime'] = pd.to_datetime(data_grouped['connectionTime'])\n",
    "\n",
    "# Drop rows where 'day_of_week' is 5 or 6\n",
    "data_grouped = data_grouped[~data_grouped['day_of_week'].isin([5, 6])]\n",
    "data_grouped['sin_day_of_week'] = np.sin(2 * np.pi * data_grouped['day_of_week'] / 10)\n",
    "data_grouped['cos_day_of_week'] = np.cos(2 * np.pi * data_grouped['day_of_week'] / 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale the kWhDelivered and cyclical features\n",
    "scaled_features = scaler.fit_transform(data_grouped[['scaled_kWhDelivered', 'sin_day_of_week', 'cos_day_of_week']].values)\n",
    "\n",
    "# Function to create dataset with look_back\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        a = dataset[i:(i + look_back), :]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Specify the look_back window\n",
    "look_back = 2\n",
    "\n",
    "# Prepare the dataset for training\n",
    "dataset = scaled_features.astype('float32')\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(dataset) * 0.90)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "\n",
    "# Reshape into X=t and Y=t+1\n",
    "X_train, Y_train = create_dataset(train, look_back)\n",
    "X_test, Y_test = create_dataset(test, look_back)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], look_back, X_train.shape[2]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], look_back, X_test.shape[2]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "from sklearn.svm import SVR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "\n",
    "def custom_mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true - y_pred), axis=-1)\n",
    "def cnn_bilstm(input_shape):\n",
    "    # Model Definition with CNN and Bidirectional LSTM\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='LeakyReLU', input_shape=(look_back, X_train.shape[2])))\n",
    "    model.add(Bidirectional(LSTM(150)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=custom_mae, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a function for each model\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(150, input_shape=input_shape))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=custom_mae, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(150, input_shape=input_shape))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=custom_mae, optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# For XGBoost and SVM, reshape data to 2D\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Train each model and get MAE\n",
    "results = {}\n",
    "\n",
    "\n",
    "\n",
    "# cnnbilstm\n",
    "cnnlstm_model = cnn_bilstm((look_back, X_train.shape[2]))\n",
    "cnnlstm_model.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0)\n",
    "cnnlstm_predict = cnnlstm_model.predict(X_test)\n",
    "cnnlstm_mae = mean_absolute_error(Y_test, cnnlstm_predict)\n",
    "results['CNNLSTM'] = cnnlstm_mae\n",
    "\n",
    "\n",
    "# LSTM\n",
    "lstm_model = create_lstm_model((look_back, X_train.shape[2]))\n",
    "lstm_model.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0)\n",
    "lstm_predict = lstm_model.predict(X_test)\n",
    "lstm_mae = mean_absolute_error(Y_test, lstm_predict)\n",
    "results['LSTM'] = lstm_mae\n",
    "\n",
    "# GRU\n",
    "gru_model = create_gru_model((look_back, X_train.shape[2]))\n",
    "gru_model.fit(X_train, Y_train, epochs=200, batch_size=1, verbose=0)\n",
    "gru_predict = gru_model.predict(X_test)\n",
    "gru_mae = mean_absolute_error(Y_test, gru_predict)\n",
    "results['GRU'] = gru_mae\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(X_train_flat, Y_train)\n",
    "xgb_predict = xgb_model.predict(X_test_flat)\n",
    "xgb_mae = mean_absolute_error(Y_test, xgb_predict)\n",
    "results['XGBoost'] = xgb_mae\n",
    "\n",
    "# SVM\n",
    "svm_model = SVR()\n",
    "svm_model.fit(X_train_flat, Y_train)\n",
    "svm_predict = svm_model.predict(X_test_flat)\n",
    "svm_mae = mean_absolute_error(Y_test, svm_predict)\n",
    "results['SVM'] = svm_mae\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "results_df = pd.DataFrame(list(results.items()), columns=['Model', 'MAE'])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23018418342207428"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_grouped['scaled_kWhDelivered'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
